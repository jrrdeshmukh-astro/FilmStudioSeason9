# FilmStudioPilot - Native Apple Stack Rules

## Core Principle: Zero Third-Party Dependencies
This app uses **only** Apple's native frameworks and APIs. No third-party UI libraries, video players, or networking libraries. All functionality should be built using Apple's official frameworks.

## Core Interaction Model: Apple TV-Like Experience
**User Experience Philosophy:**
- Users interact with the app like **Apple TV** - browse, watch, discover content
- **Productions happen automatically in the background** - no manual project creation
- **Story generation is triggered by observable media state changes** - watching, liking, skipping, pausing patterns
- Users consume finished productions, not manage pipelines
- The app observes user behavior and generates personalized content automatically

---

## Tech Stack

### Primary UI Framework
- **SwiftUI** - Main UI layer for all screens, navigation, forms, lists, dashboards, and pipeline views
  - Documentation: https://developer.apple.com/documentation/SwiftUI
  - Tutorials: https://developer.apple.com/tutorials/SwiftUI
  - App Structure: https://developer.apple.com/tutorials/swiftui-concepts/exploring-the-structure-of-a-swiftui-app
  - State & Data Flow: https://developer.apple.com/documentation/swiftui/model-data
  - Controls & Indicators: https://developer.apple.com/documentation/swiftui/controls-and-indicators

### Secondary UI Framework (Power Features)
- **UIKit** - Only when SwiftUI lacks needed functionality or for deep system integration
  - Overview: https://developer.apple.com/documentation/uikit
  - Views & Controls: https://developer.apple.com/documentation/uikit/views-and-controls
  - UIView Essentials: https://developer.apple.com/documentation/uikit/uiview
  - Interoperability: https://developer.apple.com/tutorials/swiftui/interfacing-with-uikit

### Media & Video Framework
- **AVKit** - UI for video playback, controls, PiP, chapters, subtitles
  - Framework Docs: https://developer.apple.com/documentation/avkit
  - SwiftUI VideoPlayer: https://developer.apple.com/documentation/avkit/videoplayer
  - AVPlayerViewController: https://developer.apple.com/documentation/avkit/avplayerviewcontroller
  - Picture in Picture: https://developer.apple.com/documentation/avkit/adopting-picture-in-picture-in-a-custom-player

- **AVFoundation** - Base media engine for editing, trimming, exporting (non-UI layer)
  - Programming Guide: https://developer.apple.com/library/archive/documentation/AudioVideo/Conceptual/AVFoundationPG/

### 3D & Spatial Framework
- **RealityKit** - 3D scenes, AR experiences, spatial visualization for storyboards and production planning
  - Framework Docs: https://developer.apple.com/documentation/realitykit
  - Getting Started: https://developer.apple.com/documentation/realitykit/getting-started-with-realitykit
  - SwiftUI Integration: https://developer.apple.com/documentation/realitykit/creating-3d-content-with-realitykit
  - ARView: https://developer.apple.com/documentation/realitykit/arview
  - Entity & Components: https://developer.apple.com/documentation/realitykit/entity
  - Scene Understanding: https://developer.apple.com/documentation/realitykit/scene-understanding
  - Spatial Audio: https://developer.apple.com/documentation/realitykit/spatial-audio

**Usage in App:**
- 3D storyboard visualization with stunning visuals
- AR scene preview
- Virtual set planning
- Spatial storyboard exploration
- 3D animatic generation

**Visual Components & Materials:**
- **MaterialBuilder API**: Use `PhysicallyBasedMaterial` for realistic rendering
- **Material Types**: Metallic, Fabric, Glass, Skin, Wood, Emissive
- **Visual Components**: ShadowComponent, CollisionComponent, PhysicsBodyComponent, OcclusionComponent
- **Advanced Lighting**: Three-point lighting (key, fill, rim) with shadow casting
- **Atmospheric Effects**: Fog, emissive glow, emotional moment enhancements
- **Reference**: `docs/realitykit-visuals.md` for complete visual component guide

### DirectorKit SDK (AI-Assisted Film Direction)
- **Purpose**: High-level orchestration layer for turning screenplays into directed sequences of shots
- **Core Functionality**:
  - Visual detailing (camera, framing, blocking, timing)
  - Dialogue detailing (who speaks, to whom, under what conditions)
  - Integration with Open Screenplay Format (OSF)
  - AVFoundation composition for timeline & video export
- **Data Models**:
  - `DirectorProject`: Complete film project with screenplay and directed scenes
  - `Screenplay`: Screenplay data with OSF XML support
  - `DirectedScene`: Scene broken down into shots with camera setups
  - `Shot`: Individual shot with camera, framing, dialogue, and timing
  - `AssetCatalog`: Video, audio, image, and 3D model assets
- **Services**:
  - `DirectorKitService`: Orchestrates screenplay-to-shots conversion
  - `OSFParser`: Parses and generates Open Screenplay Format XML
- **Integration Points**:
  - RealityKit: 3D scene visualization and blocking
  - AVFoundation: Timeline composition and video export
  - Production Pipeline: Automatic direction from story ideas

**Usage in App:**
- Convert story ideas to screenplays
- Generate directed scenes with camera setups
- Plan blocking and character movement
- Create AVComposition timelines
- Export final productions

### Voice Over & Dialogue Rigging
- **Purpose**: Professional voice over system with character backstory integration and dialogue timing
- **Core Functionality**:
  - Character backstory with Stanislavski objectives, relationships, and emotional range
  - Voice profile generation (pitch, pace, timbre, accent)
  - Dialogue rigging with timing, pauses, emphasis points
  - Emotional delivery based on character context
  - Acting technique integration (Stanislavski, Meisner, Method)
- **Data Models**:
  - `CharacterBackstory`: Complete character development with biography, objectives, relationships
  - `VoiceProfile`: Voice characteristics and emotional delivery patterns
  - `DialogueRigging`: Dialogue timing, pauses, emphasis, and delivery instructions
- **Services**:
  - `VoiceOverService`: Generates dialogue rigging from character backstory and dialogue
- **Integration Points**:
  - DirectorKit: Uses character backstory for dialogue shot timing
  - Acting Methods: Applies Stanislavski objectives and Meisner partner focus
  - AVFoundation: Audio timing for voice over tracks

**Usage in App:**
- Generate character backstories from story ideas
- Create voice profiles based on character traits
- Rig dialogue with proper timing and emotional delivery
- Integrate with DirectorKit for shot planning
- Export voice over tracks for final production

### Music Generation
- **Purpose**: Generate appropriate music for productions based on emotional context and scene characteristics
- **Core Functionality**:
  - Music generation based on scene emotions
  - Tempo and key mapping from emotional context
  - Instrument selection based on scene mood
  - Background music for productions
  - Sound design and atmospheres
- **Framework**: **AudioKit** (open-source, native Swift/iOS)
  - Synthesis: Oscillators, filters, effects
  - Sequencing: MIDI sequencing and playback
  - Effects: Reverb, delay, distortion, compression
  - Real-time: Low-latency audio processing
  - SwiftUI Integration: Native SwiftUI support
- **Services**:
  - `MusicGenerationService`: Generates music using AudioKit based on emotional context
- **Integration Points**:
  - Production Pipeline: Add music to productions automatically
  - Emotional Context: Map emotions to music parameters (tempo, key, instruments)
  - AVFoundation: Export generated music as audio assets
- **Reference Document**: `docs/music-generation.md` for complete integration guide

**Usage in App:**
- Generate background music for productions
- Create emotional scoring based on scene context
- Match music tempo to scene pacing
- Export music as audio assets for final production

### Design System
- **Human Interface Guidelines (HIG)** - Apple's design standards
  - HIG Hub: https://developer.apple.com/design/human-interface-guidelines
  - Designing for iOS: https://developer.apple.com/design/human-interface-guidelines/designing-for-ios
  - Components: https://developer.apple.com/design/human-interface-guidelines/components
  - Navigation & Search: https://developer.apple.com/design/human-interface-guidelines/navigation-and-search
  - Layout: https://developer.apple.com/design/human-interface-guidelines/layout
  - Color: https://developer.apple.com/design/human-interface-guidelines/color
  - Accessibility: https://developer.apple.com/design/human-interface-guidelines/accessibility

### Domain Knowledge (Acting & Film Theory)
- **Acting Methods Documentation** - Film-school style knowledge base for story generation and analysis
  - Location: `docs/acting/` directory in project root
  - Files: `stanislavski.md`, `meisner.md`, `method_strasberg.md`
  - Purpose: Enable AI to generate and analyze stories using professional acting techniques

**Stanislavski System**
- Overview: Objectives, "given circumstances," "magic if," bits/beats, superobjective
- Wikipedia: https://en.wikipedia.org/wiki/Stanislavski%27s_system
- Practical Guide: https://www.backstage.com/magazine/article/the-definitive-guide-to-the-stanislavsky-acting-technique-65716/
- Key Concepts: Objectives/obstacles, magic if, beats, inner monologue, emotional memory

**Meisner Technique**
- Overview: Repetition exercise, focus on partner, living truthfully under imaginary circumstances
- Wikipedia: https://en.wikipedia.org/wiki/Meisner_technique
- Practical Guide: https://www.backstage.com/magazine/article/the-definitive-guide-to-the-meisner-technique-67712/
- Film School Guide: https://www.nfi.edu/meisner-technique/
- Key Concepts: Repetition, moment-to-moment work, emotional preparation, partner focus

**Strasberg / Method Acting**
- Official Description: Lee Strasberg Theatre & Film Institute
- What is Method Acting: https://strasberg.edu/about/what-is-method-acting/
- Class Descriptions: https://strasberg.edu/los-angeles/about/class-descriptions/
- Key Concepts: Instrument, sense memory, affective memory, emotional recall

**Usage in App:**
- Story generation: Label scenes with objectives, obstacles, tactics, beats
- Performance notes: Direct virtual actors using Meisner/Stanislavski principles
- Scene analysis: Break down generated content using acting method frameworks
- Reference in code: `docs/acting/stanislavski.md` for story structure validation

### Observable Media State & Automatic Production
- **Media Observation** - Track user media consumption patterns to trigger story generation
- **State Changes** - Observe watching, pausing, skipping, liking, completion patterns
- **Automatic Triggers** - Story generation happens when state changes indicate new preferences
- **Background Production** - Productions are created and processed automatically, released when ready
- **Framework:** Use `@Observable` macro for media state models, Combine for state observation

**Key Observable States:**
- `MediaWatchState` - tracks what user is watching, how long, completion rate
- `MediaInteractionState` - tracks likes, skips, pauses, replays
- `MediaPreferenceState` - derived from consumption patterns
- `ProductionState` - tracks background production pipeline status

**State Change Triggers:**
- User completes a production → analyze patterns → generate new story
- User skips multiple productions → adjust taste profile → generate different style
- User watches same production multiple times → identify strong preference → generate similar content
- User pauses at specific moments → identify emotional beats → incorporate in future stories

### External APIs (Open Source Social Media for Taste Analysis)
- **Note:** These are REST APIs accessed via native `URLSession` (no third-party networking libraries)
- **Purpose:** Enrich user taste profiles from open-source social media, build taste clusters
- **No API Keys Required:** All use OAuth 2.0 (user consent) or public endpoints

**Open Source Social Media APIs (Taste Analysis)**
- **Mastodon API**: Federated social network for user interests and trending topics
  - Documentation: https://docs.joinmastodon.org/api/
  - Streaming API: https://docs.joinmastodon.org/api/streaming/
  - Use for: Hashtag analysis, timeline preferences, favorite content
- **Reddit API**: Subreddit interests and content preferences
  - Documentation: https://www.reddit.com/dev/api/
  - OAuth Guide: https://github.com/reddit-archive/reddit/wiki/OAuth2
  - Use for: Subreddit → genre mapping, post analysis, upvote patterns
- **Lemmy API**: Open-source Reddit alternative
  - Documentation: https://join-lemmy.org/api/
  - Use for: Community interests, federated content analysis
- **ActivityPub Protocol**: Universal federated social web protocol
  - Specification: https://www.w3.org/TR/activitypub/
  - Use for: Unified taste analysis across multiple platforms
- **Reference Document**: `docs/social-media-apis.md` for complete integration guide

**API Integration Architecture:**
- Use native `URLSession` for all HTTP requests
- Implement protocol-based design for multiple API support
- Cache responses appropriately using `URLCache`
- Handle rate limiting and error states gracefully
- Store API keys securely (not in code - use environment/config)

---

## Feature-to-Framework Mapping

### Onboarding & Initial Taste Setup
- **SwiftUI Forms** - Use `Form`, `Toggle`, `Slider`, `Picker` for initial preferences
- **Navigation** - Use `NavigationStack` or `NavigationSplitView`
- **State Management** - Use `@State`, `@Observable`, `@Environment`
- **Local Movie Selection** - Pre-defined popular movies list (no API needed)
- **Search Functionality** - Native SwiftUI `TextField` for movie/genre search
- **Taste Analysis** - Build initial `TasteProfile` from user selections using local genre data
- **Optional Social Media** - User can optionally connect Reddit/Mastodon accounts (OAuth)
- Reference: SwiftUI Controls & Indicators, HIG Components

### Browse & Discover (Apple TV-Like Home Screen)
- **Hero Carousel** - Use SwiftUI `TabView` with page style for featured productions
- **Content Rows** - Use `ScrollView` with `LazyHStack` for horizontal scrolling rows
- **Categories** - "For You", "Trending", "Recently Released", "Similar to What You Watched"
- **Poster Grid** - Use `LazyVGrid` with `AsyncImage` for production thumbnails
- **Navigation** - Use `NavigationLink` for deep linking into production details
- **State Observation** - Use `@Observable` to track browsing patterns
- Reference: Core SwiftUI documentation, HIG Navigation patterns, Apple TV design patterns

### Production Detail Screen
- **Hero Image/Video** - Large poster or preview video at top
- **Metadata Display** - Title, genre, duration, release date
- **Description** - Story synopsis, generated using acting method frameworks
- **Play Button** - Prominent play action (like Apple TV)
- **Related Productions** - Similar content recommendations
- **Layout** - Use `ScrollView` with `VStack` for vertical content
- Reference: HIG Layout guidelines, SwiftUI Controls

### Playback Screen (Full Screen Video Experience)
- **Video Playback** - Use SwiftUI `VideoPlayer` (AVKit) for standard playback
- **Full Screen** - Native full-screen video experience like Apple TV
- **Advanced Controls** - Use `AVPlayerViewController` wrapped in `UIViewControllerRepresentable` for system controls
- **Picture in Picture** - Implement via `AVPictureInPictureController` for background viewing
- **State Tracking** - Observe playback state (playing, paused, completed, skipped) to trigger story generation
- **Auto-play Next** - Automatically suggest next production when current completes
- Reference: AVKit documentation, SwiftUI-UIKit interoperability

### Background Production Pipeline (Automatic)
- **Production Engine** - Runs automatically based on observable media state changes
- **Pipeline Stages** - Story → Script → Storyboard → Animatic → Final Production
- **Progress Tracking** - Use `@Observable` models to track production status
- **Notification System** - Notify user when new productions are ready (using native notifications)
- **Layout** - Hidden from main UI, accessible via Settings or "My Productions" section
- Reference: HIG Layout guidelines, SwiftUI Controls, Background Tasks

### 3D Storyboard & AR Preview
- **RealityKit Views** - Use `RealityView` (SwiftUI) or `ARView` (UIKit wrapper) for 3D visualization
- **Scene Composition** - Build 3D scenes from storyboard data
- **AR Preview** - Preview scenes in AR using ARKit + RealityKit
- **Spatial Planning** - Virtual set layout and camera positioning
- **3D Animatics** - Animated 3D storyboards with camera movements
- Reference: RealityKit documentation, ARKit integration, SwiftUI-ARKit interoperability

### Settings / Preferences
- **Forms** - Use SwiftUI `Form` with `Toggle`, `Picker`, `TextField`
- **Production Preferences** - Control automatic generation frequency, genre preferences
- **Viewing History** - See what you've watched, liked, skipped
- **Navigation** - Use `NavigationStack` with settings sections
- Reference: SwiftUI Controls, HIG Components

---

## Code Style Guidelines

### SwiftUI Best Practices
1. **State Management**
   - Prefer `@Observable` macro for model classes (iOS 17+)
   - Use `@State` for local view state
   - Use `@Environment` for system values and app-wide state
   - Use `@Query` for SwiftData models

2. **View Composition**
   - Break complex views into smaller, reusable components
   - Use view modifiers for styling
   - Leverage SF Symbols for icons
   - Use system typography and colors

3. **Navigation**
   - Use `NavigationStack` for iOS 16+ (preferred)
   - Use `NavigationSplitView` for iPad/Mac multi-column layouts
   - Follow HIG navigation patterns

4. **Performance**
   - Use `LazyVStack`, `LazyHStack`, `LazyGrid` for large lists
   - Use `AsyncImage` for remote images
   - Avoid unnecessary view updates

### UIKit Integration
- Only use UIKit when SwiftUI cannot provide needed functionality
- Always wrap UIKit views/controllers in `UIViewRepresentable` or `UIViewControllerRepresentable`
- Maintain SwiftUI-first architecture

### Media Handling
- Use `VideoPlayer` for standard playback needs
- Use `AVPlayerViewController` only when advanced controls are required
- Implement PiP for background playback during editing
- Use AVFoundation for media processing (trimming, exporting, etc.)

### Design & Accessibility
- Follow HIG guidelines for spacing, typography, and colors
- Support Dark Mode using semantic colors
- Ensure all interactive elements meet accessibility standards
- Use VoiceOver labels and hints
- Test with Dynamic Type

---

## Documentation Reference Strategy

When implementing features:
1. **First** - Check SwiftUI documentation for built-in solutions
2. **Second** - Check HIG for design patterns and best practices
3. **Third** - Check AVKit/AVFoundation for media-specific needs
4. **Last Resort** - Use UIKit with SwiftUI wrappers only when necessary

Always prefer:
- Native SwiftUI components over custom implementations
- System-provided icons (SF Symbols) over custom assets
- System typography and colors over custom styling
- Built-in accessibility features over custom solutions

---

## Project Structure

```
FilmStudioPilot/
├── Views/
│   ├── Onboarding/
│   ├── Browse/ (Apple TV-like home screen)
│   │   ├── HomeView.swift
│   │   ├── ProductionRowView.swift
│   │   ├── HeroCarouselView.swift
│   │   └── CategorySectionView.swift
│   ├── ProductionDetail/
│   │   ├── ProductionDetailView.swift
│   │   └── ProductionMetadataView.swift
│   ├── Playback/
│   │   ├── VideoPlayerView.swift
│   │   └── PlaybackControlsView.swift
│   ├── RealityKit/
│   │   ├── Storyboard3DView.swift (3D scene visualization)
│   │   ├── Storyboard3DListView.swift (list of 3D scenes)
│   │   └── ARPreviewView.swift (AR preview of scenes)
│   └── Settings/
├── Models/
│   ├── Production.swift (completed productions for viewing)
│   ├── StoryIdea.swift
│   ├── Pipeline.swift (background processing stages)
│   ├── LocalMovie.swift (local movie representation, no API needed)
│   ├── TasteProfile.swift
│   ├── Observable/
│   │   ├── MediaWatchState.swift (@Observable)
│   │   ├── ProductionState.swift (@Observable)
│   │   ├── MediaInteractionState.swift (@Observable)
│   │   └── ProductionEngine.swift (@Observable)
│   ├── RealityKit/
│   │   └── Scene3D.swift (3D scene models)
│   └── DirectorKit/
│       ├── DirectorProject.swift
│       ├── Screenplay.swift
│       ├── DirectedScene.swift
│       └── AssetCatalog.swift
│   └── VoiceOver/
│       ├── CharacterBackstory.swift
│       ├── VoiceProfile.swift
│       └── DialogueRigging.swift
├── ViewModels/ (if using MVVM pattern)
├── Services/
│   ├── MediaService.swift (AVFoundation)
│   ├── AIService.swift (AI/LLM integration)
│   ├── TasteAnalysisService.swift (taste clustering & analysis from local data and social media)
│   ├── StoryAnalyzer.swift (uses acting method docs)
│   ├── ProductionEngineService.swift (background production orchestration)
│   ├── RealityKit/
│   │   └── Scene3DService.swift (3D scene generation and RealityKit entity building)
│   ├── DirectorKit/
│   │   ├── DirectorKitService.swift (screenplay-to-shots orchestration)
│   │   └── OSFParser.swift (Open Screenplay Format XML parsing)
│   ├── VoiceOver/
│   │   └── VoiceOverService.swift (dialogue rigging and voice profile generation)
│   └── SocialMedia/
│       ├── MastodonClient.swift (Mastodon API integration)
│       ├── RedditClient.swift (Reddit API integration)
│       ├── LemmyClient.swift (Lemmy API integration)
│       └── SocialMediaTasteService.swift (aggregate taste analysis from social data)
│   └── Music/
│       └── MusicGenerationService.swift (AudioKit-based music generation)
├── Observers/
│   ├── MediaStateObserver.swift (observes MediaWatchState changes)
│   ├── ProductionTriggerObserver.swift (triggers story generation)
│   └── TasteProfileObserver.swift (updates taste profile from state)
├── Network/
│   ├── APIClient.swift (base URLSession wrapper for social media APIs)
│   └── SocialMedia/
│       ├── RedditClient.swift (Reddit API - OAuth, no key needed)
│       ├── MastodonClient.swift (Mastodon API - OAuth, no key needed)
│       └── LemmyClient.swift (Lemmy API - OAuth, no key needed)
├── Utilities/
│   ├── Extensions/
│   └── Helpers/
└── docs/ (domain knowledge for AI context)
    ├── acting/
    │   ├── stanislavski.md
    │   ├── meisner.md
    │   └── method_strasberg.md
    ├── social-media-apis.md (open source social media API integration guide)
    ├── music-generation.md (AudioKit music generation guide)
    └── realitykit-visuals.md (RealityKit visual components guide)
```

---

## API Integration Patterns

### Network Layer Architecture

**Base API Client Protocol**
- Create a protocol-based design to support multiple movie APIs
- Use native `URLSession` for all HTTP requests
- Implement proper error handling and retry logic
- Support async/await patterns (iOS 15+)

**Example Structure:**
```swift
protocol SocialMediaClient {
    func getUserInterests(userId: String) async throws -> [Interest]
    func getTrendingTopics() async throws -> [Topic]
}

// Reddit implementation (OAuth, no API key)
class RedditClient: SocialMediaClient {
    private let accessToken: String // From OAuth
    private let baseURL = "https://oauth.reddit.com"
    
    func getUserInterests(userId: String) async throws -> [Interest] {
        // URLSession with OAuth token
    }
}

// Mastodon implementation (OAuth, no API key)
class MastodonClient: SocialMediaClient {
    private let instanceURL: String
    private let accessToken: String // From OAuth
    
    func getUserInterests(userId: String) async throws -> [Interest] {
        // URLSession with OAuth token
    }
}
```

**Data Models**
- Create Swift `Codable` structs matching API responses
- Use `CodingKeys` for API naming conventions (snake_case → camelCase)
- Handle optional fields gracefully
- Map to unified app models (`MovieMetadata`, `TasteProfile`)

**Error Handling**
- Define custom error types for API failures
- Handle rate limiting (429 responses)
- Implement exponential backoff for retries
- Provide user-friendly error messages

**Caching Strategy**
- Use `URLCache` for HTTP response caching
- Cache trending data for short periods (15-30 minutes)
- Cache movie details longer (24 hours)
- Invalidate cache on user actions (new likes, etc.)

### Taste Analysis Architecture

**TasteProfile Model**
- Track user's liked movies, genres, themes
- Build taste vectors from social media data (subreddits, hashtags, communities)
- Cluster similar tastes using genre/keyword similarity
- Update profile based on user interactions and social media activity

**Social Media Taste Engine**
- Analyze user's social media activity (Reddit subreddits, Mastodon hashtags)
- Map social interests to movie genres
- Build taste clusters (genre-based, theme-based, mood-based)
- Generate story prompts aligned with user's taste profile

**Integration Points:**
- Onboarding: User manually selects favorite movies/genres (no API needed)
- Social Media: Connect accounts to enhance taste profile
- Story Generation: Reference taste clusters when generating ideas
- Discovery: Show productions matching user's taste profile

### OAuth & Authentication (Social Media APIs)

**OAuth 2.0 Implementation:**
- Use native `ASWebAuthenticationSession` for OAuth flows
- Store access tokens securely in Keychain
- No API keys needed - user grants permission via OAuth
- User can revoke access at any time

**Best Practices:**
```swift
// Example: OAuth flow with ASWebAuthenticationSession
let authURL = URL(string: "https://oauth.reddit.com/authorize?...")!
let session = ASWebAuthenticationSession(
    url: authURL,
    callbackURLScheme: "filmstudiopilot"
) { callbackURL, error in
    // Handle OAuth callback, extract access token
    // Store in Keychain
}
```

**Rate Limiting:**
- Respect API rate limits (varies by platform)
- Implement request queuing/throttling
- Cache responses to minimize API calls
- Handle rate limit errors gracefully

**Error Handling:**
- Handle 401 (unauthorized) - token expired, re-authenticate
- Handle 429 (too many requests) - rate limit exceeded
- Handle network errors gracefully
- Provide fallback behavior when APIs are unavailable

---

## Domain Knowledge Integration

### Using Acting Method Documentation

**File Structure:**
- Store acting method docs in `docs/acting/` directory
- Each file should contain:
  - Key concepts and definitions
  - Core exercises and techniques
  - Practical application guidelines
  - Scene analysis frameworks

**Integration in Code:**

**Story Generation:**
```swift
// StoryAnalyzer.swift should reference acting method docs
// When generating scenes, ensure each has:
// - Clear objective (Stanislavski)
// - Obstacles and tactics
// - Beats (scene breakdown)
// - Character motivations
```

**Performance Notes:**
```swift
// When generating performance direction:
// - Use Meisner focus on partner behavior
// - Apply Stanislavski emotional memory techniques
// - Reference Method Acting sense memory when appropriate
```

**Scene Analysis:**
```swift
// Analyze generated content using acting frameworks:
// - Break scenes into beats
// - Identify character objectives
// - Map obstacles and tactics
// - Validate emotional truth (Meisner)
```

**Prompt Engineering:**
- Reference `docs/acting/stanislavski.md` in AI prompts for story structure
- Use `docs/acting/meisner.md` for performance direction
- Apply `docs/acting/method_strasberg.md` for deep character work
- Example: "Generate a scene following Stanislavski principles: each character must have a clear objective, obstacle, and tactics. Break the scene into beats per the guidelines in `docs/acting/stanislavski.md`."

**Best Practices:**
- Keep doc files concise (summaries, not full copies)
- Use headings and bullet points for easy parsing
- Include practical examples and exercises
- Update docs as new acting techniques are integrated
- Reference docs in code comments when applying techniques

**Markdown File Structure Template:**
```markdown
# [Acting Method Name]

## Core Principles
- Key concept 1
- Key concept 2

## Key Techniques
### Technique Name
- Description
- How to apply
- Example usage

## Scene Analysis Framework
- Step 1: Identify...
- Step 2: Map...
- Step 3: Validate...

## Practical Application
- Exercise 1
- Exercise 2
```

**Content Guidelines:**
- Summarize in your own words (avoid copyright issues)
- Focus on actionable concepts for story generation
- Include scene breakdown frameworks
- Provide clear examples of application
- Keep each file under 500-1000 words for easy AI parsing

---

## Automatic Production Workflow (Background Process)

### Observable Media State Architecture

**State Observation Pattern:**
```swift
@Observable
class MediaWatchState {
    var currentProduction: Production?
    var watchProgress: Double = 0.0
    var isPlaying: Bool = false
    var completedProductions: [Production] = []
    var skippedProductions: [Production] = []
    var likedProductions: [Production] = []
    var pauseMoments: [TimeInterval] = []
}

@Observable
class ProductionEngine {
    var activeProductions: [Production] = []
    var queuedStories: [StoryIdea] = []
    
    func observeStateChanges(_ state: MediaWatchState) {
        // Trigger story generation based on state changes
    }
}
```

### State Change Triggers & Story Generation

**Trigger 1: Production Completion**
- User finishes watching a production
- State change: `watchProgress` reaches 1.0, `completedProductions` updated
- Action: Analyze completion patterns → Generate similar story
- Implementation: `ProductionEngine` observes `MediaWatchState.completedProductions` changes

**Trigger 2: Multiple Skips**
- User skips 3+ productions in a row
- State change: `skippedProductions` count increases
- Action: Adjust taste profile → Generate different genre/style
- Implementation: Observe `skippedProductions` array changes

**Trigger 3: Repeat Viewing**
- User watches same production multiple times
- State change: Production appears multiple times in watch history
- Action: Identify strong preference → Generate similar content
- Implementation: Track production view count in `MediaWatchState`

**Trigger 4: Pause Pattern Analysis**
- User pauses at specific emotional moments
- State change: `pauseMoments` array populated
- Action: Identify emotional beats → Incorporate in future stories
- Implementation: Observe `pauseMoments` changes, analyze timing patterns

**Trigger 5: Like/Dislike Patterns**
- User likes certain productions, skips others
- State change: `likedProductions` vs `skippedProductions` ratio
- Action: Refine taste profile → Generate aligned content
- Implementation: Compare liked vs skipped patterns

### Automatic Production Pipeline

**Stage 1: Story Generation (Triggered by State Change)**
1. `ProductionEngine` detects state change
2. `StoryAnalyzer` references `docs/acting/stanislavski.md`
3. `AIService` generates story using:
   - Current `TasteProfile` (updated from state observations)
   - Acting method frameworks
   - State change context (what triggered generation)
4. Story added to `queuedStories`

**Stage 2: Background Processing**
- Production moves through pipeline automatically:
  - Story → Script → Storyboard → Animatic → Final Production
- Each stage processes in background
- Status tracked via `@Observable` `ProductionState`
- User not interrupted during viewing

**Stage 3: Release for Viewing**
- When production completes, added to "Recently Released" category
- User sees new content in browse feed
- Notification sent (optional, user preference)
- Production becomes available for viewing

### User Experience Flow

**Primary Flow (Apple TV-Like):**
1. User opens app → Sees browse feed (like Apple TV home)
2. User browses categories → "For You", "Trending", "New Releases"
3. User taps production → Sees detail screen
4. User taps "Play" → Full-screen video playback
5. User watches → State changes observed automatically
6. User completes/skips → Triggers background story generation
7. New productions appear → User discovers in feed (no manual creation)

**Background Flow (Automatic):**
1. State change detected → `ProductionEngine` triggered
2. Story generated → Based on observable patterns
3. Production pipeline → Processes automatically
4. Production completes → Released to browse feed
5. User discovers → In next browsing session

### Implementation Notes

**State Observation:**
- Use `@Observable` macro for reactive state models
- Use Combine publishers for cross-component state observation
- Implement `onChange` modifiers to react to state changes
- Background tasks for production processing (using `BGTaskScheduler`)

**Performance:**
- Production processing happens in background
- State observation is lightweight and efficient
- Cache generated stories to avoid regeneration
- Batch state changes to avoid excessive triggers

**User Control:**
- Settings allow users to control generation frequency
- Users can pause automatic generation
- Users can view "My Productions" to see what's being created
- Users can manually trigger generation if desired

---

## Import Statements

Standard imports for this project:
```swift
import SwiftUI          // Primary UI
import SwiftData        // Data persistence
import AVKit            // Video playback UI
import AVFoundation      // Media processing
import CoreMedia        // Media timeline and composition
import RealityKit       // 3D scenes and AR experiences
import ARKit            // AR capabilities (when using ARView)
import Combine          // State observation and reactive programming
import UIKit            // Only when needed for UIKit integration
import BackgroundTasks  // Background production processing
import Foundation       // XML parsing and utilities
```

---

## Testing Considerations

- Test on multiple iOS versions (minimum iOS 16+ for modern SwiftUI features)
- Test on iPhone and iPad (use `NavigationSplitView` for adaptive layouts)
- Test Dark Mode and Light Mode
- Test with Dynamic Type (accessibility)
- Test Picture in Picture functionality
- Test video playback in various scenarios

---

## When to Reference Documentation

### Apple Framework Docs
- **Building new screens** → SwiftUI Tutorials, HIG Components
- **State management issues** → SwiftUI Model Data documentation
- **Navigation problems** → HIG Navigation & Search, SwiftUI NavigationStack
- **Video playback** → AVKit documentation, VideoPlayer
- **Media processing** → AVFoundation Programming Guide
- **Design questions** → HIG Layout, Color, Typography
- **Accessibility** → HIG Accessibility guidelines
- **UIKit integration** → SwiftUI-UIKit Interoperability tutorial

### Domain Knowledge & APIs
- **Story generation** → `docs/acting/stanislavski.md` for scene structure (objectives, obstacles, beats)
- **Performance direction** → `docs/acting/meisner.md` for actor guidance
- **Character development** → `docs/acting/method_strasberg.md` for deep character work
- **Scene analysis** → All acting method docs for breaking down generated content
- **Social Media API integration** → Reddit API docs, Mastodon API docs (see `docs/social-media-apis.md`)
- **Taste analysis** → Social media data analysis patterns
- **OAuth implementation** → ASWebAuthenticationSession documentation
- **API error handling** → URLSession documentation, HTTP status code handling
- **DirectorKit** → Open Screenplay Format (OSF) specification: https://github.com/OpenScreenplayFormat/osf-sdk
- **AVFoundation Composition** → AVFoundation Programming Guide, AVMutableComposition documentation
- **Timeline editing** → Core Media framework for time-based media composition

### Observable State & Reactive Patterns
- **State observation** → SwiftUI `@Observable` macro, `onChange` modifier
- **Reactive programming** → Combine framework for state observation
- **Background tasks** → `BGTaskScheduler` for production processing
- **State management** → SwiftUI Model Data documentation for `@Observable`
- **Media state tracking** → AVKit playback state observation
- **Pattern recognition** → Combine operators for analyzing state changes

---

## Anti-Patterns to Avoid

❌ **Don't** use third-party UI libraries (Alamofire, Kingfisher, etc.)
❌ **Don't** use third-party video players
❌ **Don't** create custom UI components when SwiftUI provides them
❌ **Don't** ignore HIG guidelines
❌ **Don't** skip accessibility features
❌ **Don't** use UIKit when SwiftUI can handle it
❌ **Don't** hardcode colors - use semantic colors
❌ **Don't** ignore Dark Mode support

✅ **Do** use native SwiftUI components
✅ **Do** follow HIG design patterns
✅ **Do** implement accessibility from the start
✅ **Do** use system icons and typography
✅ **Do** test on multiple device sizes
✅ **Do** support Dark Mode
✅ **Do** use semantic colors

---

## Example User Workflow (Apple TV-Like Experience)

### Day 1: First Launch & Onboarding

**Step 1: Initial Setup**
- User opens app for first time
- Onboarding screen: Select 10-15 movies they like
- App builds initial `TasteProfile` from selections
- User enters main browse screen

**Step 2: Browse Feed (Like Apple TV)**
- Home screen shows:
  - **Hero Carousel**: Featured productions
  - **For You**: Personalized recommendations
  - **Trending**: Popular productions
  - **New Releases**: Recently completed productions
- User scrolls through rows, taps on interesting production

**Step 3: Production Detail**
- User sees production detail screen:
  - Large poster/thumbnail
  - Title, genre, duration
  - Story synopsis (generated using acting methods)
  - "Play" button (prominent, like Apple TV)
- User taps "Play"

**Step 4: Watch Production**
- Full-screen video playback (AVKit `VideoPlayer`)
- User watches entire production
- **State Observation Happening:**
  - `MediaWatchState.watchProgress` updates
  - `MediaWatchState.isPlaying` = true
  - Playback position tracked

**Step 5: Production Completes**
- User finishes watching
- **State Change Detected:**
  - `watchProgress` reaches 1.0
  - Production added to `completedProductions`
  - `ProductionTriggerObserver` detects change

**Step 6: Automatic Story Generation (Background)**
- `ProductionEngine` triggered by state change
- Analyzes completed production patterns
- Generates new story aligned with user's taste
- Story added to background production pipeline
- User sees "Next Up" suggestion (auto-play next)

### Day 2: Discovery & Pattern Building

**Step 7: Browse New Content**
- User opens app next day
- Sees new productions in "For You" section
- Some generated from previous day's viewing patterns

**Step 8: Skip Pattern**
- User skips 2 productions quickly
- **State Change:**
  - Productions added to `skippedProductions`
  - `MediaInteractionState` updated
- **Automatic Response:**
  - `TasteProfileObserver` adjusts preferences
  - Next generation will avoid skipped styles

**Step 9: Like & Pause Patterns**
- User finds production they like
- Pauses at emotional moments (3 times)
- **State Observation:**
  - `pauseMoments` array populated: [0:45, 1:23, 2:15]
  - Production added to `likedProductions`
- **Automatic Analysis:**
  - Emotional beats identified
  - Similar beats will be incorporated in future stories

**Step 10: Background Production**
- While user browses, production engine:
  - Processes story from Day 1 completion
  - Moves through: Script → Storyboard → Animatic
  - Status tracked but hidden from user
  - User continues browsing/watching

### Day 3: New Release Discovery

**Step 11: Production Released**
- Background production completes
- Added to "New Releases" category
- User discovers it naturally while browsing
- No notification (unless user enabled)

**Step 12: Repeat Viewing**
- User watches same production twice
- **State Change:**
  - Production view count = 2
  - Strong preference signal
- **Automatic Response:**
  - `ProductionEngine` generates similar story
  - Same genre, similar themes, matching emotional beats

### Ongoing: Continuous Refinement

**Step 13: Pattern Recognition**
- App observes:
  - User always watches sci-fi to completion
  - User skips romance productions
  - User pauses at action sequences
- **Automatic Adaptation:**
  - Taste profile refined continuously
  - Future productions align with patterns
  - User never manually adjusts anything

**Step 14: Perfect Personalization**
- After weeks of use:
  - Every production in "For You" is highly relevant
  - User rarely skips
  - Productions match emotional preferences
  - All driven by observable state changes

### Key Differences from Manual Workflow

**Old Model (Manual):**
- User creates project
- User manages pipeline
- User writes story
- User edits script
- User reviews each stage

**New Model (Apple TV-Like):**
- User browses and watches
- Productions happen automatically
- Stories generated from state changes
- User discovers finished content
- Zero manual project management

---

## Quick Reference Links

### Apple Frameworks

**SwiftUI**
- Main Docs: https://developer.apple.com/documentation/SwiftUI
- Tutorials: https://developer.apple.com/tutorials/SwiftUI

**AVKit**
- Framework: https://developer.apple.com/documentation/avkit
- VideoPlayer: https://developer.apple.com/documentation/avkit/videoplayer

**HIG**
- Main Hub: https://developer.apple.com/design/human-interface-guidelines
- iOS Design: https://developer.apple.com/design/human-interface-guidelines/designing-for-ios

**AVFoundation**
- Programming Guide: https://developer.apple.com/library/archive/documentation/AudioVideo/Conceptual/AVFoundationPG/

### External APIs

**Reddit API**
- API Documentation: https://www.reddit.com/dev/api/
- OAuth Guide: https://github.com/reddit-archive/reddit/wiki/OAuth2
- App Registration: https://www.reddit.com/prefs/apps

**Mastodon API**
- API Documentation: https://docs.joinmastodon.org/api/
- Streaming API: https://docs.joinmastodon.org/api/streaming/
- Instance List: https://joinmastodon.org/servers

**ActivityPub Protocol**
- Specification: https://www.w3.org/TR/activitypub/
- ActivityStreams: https://www.w3.org/TR/activitystreams-core/

### Domain Knowledge (Acting Methods)

**Stanislavski System**
- Wikipedia: https://en.wikipedia.org/wiki/Stanislavski%27s_system
- Backstage Guide: https://www.backstage.com/magazine/article/the-definitive-guide-to-the-stanislavsky-acting-technique-65716/

**Meisner Technique**
- Wikipedia: https://en.wikipedia.org/wiki/Meisner_technique
- Backstage Guide: https://www.backstage.com/magazine/article/the-definitive-guide-to-the-meisner-technique-67712/
- NY Film Institute: https://www.nfi.edu/meisner-technique/

**Method Acting (Strasberg)**
- Official Site: https://strasberg.edu/about/what-is-method-acting/
- Class Descriptions: https://strasberg.edu/los-angeles/about/class-descriptions/

---

This rules file ensures all code suggestions and implementations align with Apple's native frameworks and design guidelines, maintaining a zero third-party dependency architecture.

